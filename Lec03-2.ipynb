{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "w1 = 1.0\n",
    "w2 = 1.0\n",
    "b = 0.1\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return x**2 * w2 + x * w1 + b\n",
    "\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y)**2\n",
    "\n",
    "def gradient1(x, y):\n",
    "    return 2*x*(x*w1 - y + w2*x**2 + b)\n",
    "\n",
    "def gradient2(x, y):\n",
    "    return 2*x**2*(x**2*w2 - y + w1*x + b)\n",
    "\n",
    "def gradientb(x, y):\n",
    "    return 2*(b - y + w2 * x **2 + w1 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 20.060000000000002\n",
      "progress: 0 w= 0.6570824959999999 0.062069887999999906 loss= 12.242112885930664\n",
      "progress: 1 w= 0.846294756438016 0.48600432522444803 loss= 0.9602538295232872\n",
      "progress: 2 w= 0.7876797049878832 0.2776394973038291 loss= 1.1692463078063395\n",
      "progress: 3 w= 0.8432630900228033 0.36390583163337753 loss= 0.010075352248462338\n",
      "progress: 4 w= 0.8446385498620044 0.3134891573349142 loss= 0.2870778056454827\n",
      "progress: 5 w= 0.8702064307388598 0.3270628243908157 loss= 0.09796831616278591\n",
      "progress: 6 w= 0.8835271035857752 0.31123901039074386 loss= 0.15780538261067992\n",
      "progress: 7 w= 0.9015940761892774 0.3094562057272633 loss= 0.11517689213982758\n",
      "progress: 8 w= 0.916537109264741 0.30148861591726667 loss= 0.1213508776188307\n",
      "progress: 9 w= 0.9320535644110748 0.29673377665411493 loss= 0.10687686919160626\n",
      "progress: 10 w= 0.9464570688216152 0.29080610261205475 loss= 0.10259336002746511\n",
      "progress: 11 w= 0.9605649704324997 0.28573375089371544 loss= 0.09451302061596223\n",
      "progress: 12 w= 0.9740283809186782 0.28056106544346215 loss= 0.08885316721257608\n",
      "progress: 13 w= 0.9870398438076522 0.27572121116496706 loss= 0.08276346694942917\n",
      "progress: 14 w= 0.9995388088606907 0.2710015833271636 loss= 0.07747696620878938\n",
      "progress: 15 w= 1.0115813674186058 0.266490478920125 loss= 0.07239447397139825\n",
      "progress: 16 w= 1.0231682590844065 0.2621365969805087 loss= 0.06774433847281683\n",
      "progress: 17 w= 1.0343249413526223 0.25795398581900286 loss= 0.0633856980094476\n",
      "progress: 18 w= 1.0450643735411829 0.2539266664723654 loss= 0.05934665178040143\n",
      "progress: 19 w= 1.0554043718363204 0.25005299644153023 loss= 0.05558187342748681\n",
      "progress: 20 w= 1.065359571083561 0.24632501328621942 loss= 0.05208150710626635\n",
      "progress: 21 w= 1.0749452005476905 0.24273803243014394 loss= 0.04882196459722715\n",
      "progress: 22 w= 1.0841753612946425 0.23928616830211105 loss= 0.04578791213427025\n",
      "progress: 23 w= 1.0930638606284482 0.2359644058076489 loss= 0.0429622135951916\n",
      "progress: 24 w= 1.1016238554669655 0.2327676244367723 loss= 0.04033031751692691\n",
      "progress: 25 w= 1.1098680486921 0.2296910407658185 loss= 0.03787814807125559\n",
      "progress: 26 w= 1.1178086270274932 0.22672999143242384 loss= 0.035592933072706626\n",
      "progress: 27 w= 1.1254573180528105 0.2238800234163484 loss= 0.033462717478597234\n",
      "progress: 28 w= 1.1328253906979604 0.2211368417453445 loss= 0.031476477990730875\n",
      "progress: 29 w= 1.1399236809917064 0.2184963239382008 loss= 0.029623974482390375\n",
      "progress: 30 w= 1.1467626050556254 0.21595450377733058 loss= 0.02789572740309054\n",
      "progress: 31 w= 1.1533521770676924 0.21350756971960114 loss= 0.026282945079056194\n",
      "progress: 32 w= 1.1597020239857336 0.21115185684921242 loss= 0.024777479846634462\n",
      "progress: 33 w= 1.1658214008865722 0.2088838421749704 loss= 0.02337177692503355\n",
      "progress: 34 w= 1.1717192051612297 0.2067001386978356 loss= 0.022058831887933134\n",
      "progress: 35 w= 1.1774039904180733 0.20459749036445418 loss= 0.02083214917158951\n",
      "progress: 36 w= 1.1828839797293789 0.20257276691011694 loss= 0.019685704664320517\n",
      "progress: 37 w= 1.1881670784215457 0.20062295904318014 loss= 0.018613910658875937\n",
      "progress: 38 w= 1.1932608863452374 0.1987451737494136 loss= 0.017611583627378514\n",
      "progress: 39 w= 1.1981727096834003 0.1969366298085798 loss= 0.01667391431799934\n",
      "progress: 40 w= 1.202909572297401 0.19519465346993994 loss= 0.01579644012892451\n",
      "progress: 41 w= 1.2074782266373305 0.19351667430153052 loss= 0.014975019529232862\n",
      "progress: 42 w= 1.2118851642295003 0.19190022119673175 loss= 0.014205808399337052\n",
      "progress: 43 w= 1.216136625759244 0.1903429185365925 loss= 0.01348523813450703\n",
      "progress: 44 w= 1.2202386107638366 0.18884248249978228 loss= 0.012809995384338174\n",
      "progress: 45 w= 1.2241968869509812 0.18739671751544146 loss= 0.012177003302710598\n",
      "progress: 46 w= 1.228016999157155 0.18600351285295985 loss= 0.011583404195809536\n",
      "progress: 47 w= 1.2317042779598124 0.1846608393435992 loss= 0.011026543462666039\n",
      "progress: 48 w= 1.235263847956782 0.18336674622876598 loss= 0.010503954731371737\n",
      "progress: 49 w= 1.2387006357257369 0.18211935813009067 loss= 0.010013346101069935\n",
      "progress: 50 w= 1.2420193774760937 0.18091687213658042 loss= 0.009552587406792611\n",
      "progress: 51 w= 1.2452246264052254 0.17975755500433366 loss= 0.009119698430367364\n",
      "progress: 52 w= 1.2483207597704165 0.1786397404644629 loss= 0.008712837986457346\n",
      "progress: 53 w= 1.2513119856875436 0.17756182663504405 loss= 0.008330293818117047\n",
      "progress: 54 w= 1.2542023496670396 0.1765222735330749 loss= 0.007970473241208883\n",
      "progress: 55 w= 1.2569957408972965 0.17551960068257394 loss= 0.007631894481586322\n",
      "progress: 56 w= 1.259695898285264 0.17455238481511023 loss= 0.007313178653174684\n",
      "progress: 57 w= 1.262306416263627 0.1736192576591873 loss= 0.007013042328992011\n",
      "progress: 58 w= 1.2648307503735832 0.17271890381505023 loss= 0.006730290660760912\n",
      "progress: 59 w= 1.2672722226318887 0.17185005871161488 loss= 0.006463811006094971\n",
      "progress: 60 w= 1.2696340266905115 0.17101150664234588 loss= 0.0062125670253414395\n",
      "progress: 61 w= 1.2719192327969058 0.17020207887703054 loss= 0.005975593213002148\n",
      "progress: 62 w= 1.2741307925626097 0.16942065184651928 loss= 0.005751989831300608\n",
      "progress: 63 w= 1.2762715435475767 0.1686661453976122 loss= 0.005540918215896862\n",
      "progress: 64 w= 1.2783442136673564 0.16793752111537688 loss= 0.005341596426015352\n",
      "progress: 65 w= 1.2803514254299764 0.16723378071029954 loss= 0.005153295213315922\n",
      "progress: 66 w= 1.2822957000090995 0.16655396446775644 loss= 0.00497533428579294\n",
      "progress: 67 w= 1.2841794611597868 0.1658971497574031 loss= 0.0048070788447420725\n",
      "progress: 68 w= 1.2860050389829485 0.16526244960016268 loss= 0.004647936374499479\n",
      "progress: 69 w= 1.2877746735443276 0.16464901129058884 loss= 0.004497353666175221\n",
      "progress: 70 w= 1.2894905183536394 0.16405601507246448 loss= 0.004354814058009207\n",
      "progress: 71 w= 1.2911546437092691 0.16348267286557644 loss= 0.004219834876285267\n",
      "progress: 72 w= 1.2927690399137248 0.16292822704169013 loss= 0.004091965061937984\n",
      "progress: 73 w= 1.2943356203648384 0.16239194924782577 loss= 0.003970782969099426\n",
      "progress: 74 w= 1.2958562245275151 0.1618731392750022 loss= 0.003855894322873345\n",
      "progress: 75 w= 1.2973326207906495 0.16137112397069567 loss= 0.0037469303245608915\n",
      "progress: 76 w= 1.2987665092136456 0.16088525619332592 loss= 0.00364354589344812\n",
      "progress: 77 w= 1.3001595241668031 0.16041491380713882 loss= 0.0035454180350908957\n",
      "progress: 78 w= 1.3015132368696765 0.1599594987159322 loss= 0.0034522443267631774\n",
      "progress: 79 w= 1.3028291578313442 0.15951843593411977 loss= 0.0033637415114504622\n",
      "progress: 80 w= 1.3041087391963841 0.15909117269368878 loss= 0.0032796441924067684\n",
      "progress: 81 w= 1.3053533770001926 0.15867717758567002 loss= 0.00319970362088267\n",
      "progress: 82 w= 1.3065644133371546 0.15827593973478069 loss= 0.0031236865701967167\n",
      "progress: 83 w= 1.3077431384450318 0.15788696800596255 loss= 0.0030513742898151627\n",
      "progress: 84 w= 1.308890792708805 0.15750979024157855 loss= 0.002982561533592848\n",
      "progress: 85 w= 1.3100085685870855 0.1571439525280888 loss= 0.0029170556567468765\n",
      "progress: 86 w= 1.311097612464083 0.1567890184910612 loss= 0.0028546757765533165\n",
      "progress: 87 w= 1.3121590264300125 0.15644456861742567 loss= 0.0027952519921172745\n",
      "progress: 88 w= 1.3131938699927017 0.15611019960391728 loss= 0.002738624658919552\n",
      "progress: 89 w= 1.3142031617230587 0.15578552373069723 loss= 0.002684643714157758\n",
      "progress: 90 w= 1.315187880836954 0.15547016825917784 loss= 0.0026331680491985033\n",
      "progress: 91 w= 1.3161489687159775 0.15516377485311705 loss= 0.0025840649257234134\n",
      "progress: 92 w= 1.31708733036943 0.15486599902208406 loss= 0.0025372094324133555\n",
      "progress: 93 w= 1.3180038358398178 0.15457650958642788 loss= 0.0024924839792426733\n",
      "progress: 94 w= 1.3188993215540405 0.15429498816292458 loss= 0.002449777826669181\n",
      "progress: 95 w= 1.3197745916223618 0.1540211286702954 loss= 0.0024089866472173376\n",
      "progress: 96 w= 1.3206304190871883 0.15375463685383722 loss= 0.002370012117119281\n",
      "progress: 97 w= 1.3214675471235917 0.15349522982841926 loss= 0.002332761535866464\n",
      "progress: 98 w= 1.3222866901934418 0.15324263563914048 loss= 0.0022971474716738786\n",
      "progress: 99 w= 1.323088535154939 0.15299659283896352 loss= 0.002263087431005687\n",
      "predict (before training) 4 8.346492766514704\n"
     ]
    }
   ],
   "source": [
    "# before training\n",
    "print(\"predict (before training)\", 4, forward(4))\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        grad1 = gradient1(x_val, y_val)\n",
    "        grad2 = gradient2(x_val, y_val)\n",
    "        gradb = gradientb(x_val, y_val)\n",
    "\n",
    "        w1 = w1 - learning_rate * grad1\n",
    "        w2 = w2 - learning_rate * grad2\n",
    "        b = b - learning_rate * gradb\n",
    "\n",
    "#         print(\"\\tgrad:\", x_val, y_val, grad1, grad2)\n",
    "        l = loss(x_val, y_val)\n",
    "        \n",
    "    print(\"progress:\", epoch, \"w=\", w1, w2, \"loss=\", l)\n",
    "\n",
    "# After training\n",
    "print(\"predict (before training)\", 4, forward(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
