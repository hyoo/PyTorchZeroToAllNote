{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "learning_rate = 0.01\n",
    "w1 = Variable(torch.Tensor([1.0]), requires_grad = True)\n",
    "w2 = Variable(torch.Tensor([1.0]), requires_grad = True)\n",
    "b = Variable(torch.Tensor([0.1]), requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return x**2 * w2 + x * w1 + b\n",
    "\n",
    "def loss(x,y):\n",
    "    return (forward(x) - y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 8.31663990020752\n",
      "\tgrad: 1.0 2.0 0.1903243064880371 0.1903243064880371 0.1903243064880371\n",
      "\tgrad: 2.0 4.0 -0.4964027404785156 -0.9928054809570312 -0.2482013702392578\n",
      "\tgrad: 3.0 6.0 0.27019214630126953 0.8105764389038086 0.09006404876708984\n",
      "progress: 0 loss= 0.002027883194386959\n",
      "\tgrad: 1.0 2.0 0.19023609161376953 0.19023609161376953 0.19023609161376953\n",
      "\tgrad: 2.0 4.0 -0.4960899353027344 -0.9921798706054688 -0.2480449676513672\n",
      "\tgrad: 3.0 6.0 0.2700176239013672 0.8100528717041016 0.09000587463378906\n",
      "progress: 1 loss= 0.0020252643153071404\n",
      "\tgrad: 1.0 2.0 0.19014692306518555 0.19014692306518555 0.19014692306518555\n",
      "\tgrad: 2.0 4.0 -0.49578285217285156 -0.9915657043457031 -0.24789142608642578\n",
      "\tgrad: 3.0 6.0 0.26982879638671875 0.8094863891601562 0.08994293212890625\n",
      "progress: 2 loss= 0.0020224328618496656\n",
      "\tgrad: 1.0 2.0 0.19005775451660156 0.19005775451660156 0.19005775451660156\n",
      "\tgrad: 2.0 4.0 -0.49547290802001953 -0.9909458160400391 -0.24773645401000977\n",
      "\tgrad: 3.0 6.0 0.2696571350097656 0.8089714050292969 0.08988571166992188\n",
      "progress: 3 loss= 0.002019860316067934\n",
      "\tgrad: 1.0 2.0 0.18996715545654297 0.18996715545654297 0.18996715545654297\n",
      "\tgrad: 2.0 4.0 -0.49516868591308594 -0.9903373718261719 -0.24758434295654297\n",
      "\tgrad: 3.0 6.0 0.2694740295410156 0.8084220886230469 0.08982467651367188\n",
      "progress: 4 loss= 0.0020171180367469788\n",
      "\tgrad: 1.0 2.0 0.18987655639648438 0.18987655639648438 0.18987655639648438\n",
      "\tgrad: 2.0 4.0 -0.4948606491088867 -0.9897212982177734 -0.24743032455444336\n",
      "\tgrad: 3.0 6.0 0.26929664611816406 0.8078899383544922 0.08976554870605469\n",
      "progress: 5 loss= 0.002014463534578681\n",
      "\tgrad: 1.0 2.0 0.18978500366210938 0.18978500366210938 0.18978500366210938\n",
      "\tgrad: 2.0 4.0 -0.4945554733276367 -0.9891109466552734 -0.24727773666381836\n",
      "\tgrad: 3.0 6.0 0.2691221237182617 0.8073663711547852 0.0897073745727539\n",
      "progress: 6 loss= 0.0020118532702326775\n",
      "\tgrad: 1.0 2.0 0.18969297409057617 0.18969297409057617 0.18969297409057617\n",
      "\tgrad: 2.0 4.0 -0.49425506591796875 -0.9885101318359375 -0.24712753295898438\n",
      "\tgrad: 3.0 6.0 0.2689476013183594 0.8068428039550781 0.08964920043945312\n",
      "progress: 7 loss= 0.002009244868531823\n",
      "\tgrad: 1.0 2.0 0.18959999084472656 0.18959999084472656 0.18959999084472656\n",
      "\tgrad: 2.0 4.0 -0.49395179748535156 -0.9879035949707031 -0.24697589874267578\n",
      "\tgrad: 3.0 6.0 0.26877307891845703 0.8063192367553711 0.08959102630615234\n",
      "progress: 8 loss= 0.0020066380966454744\n",
      "\tgrad: 1.0 2.0 0.18950700759887695 0.18950700759887695 0.18950700759887695\n",
      "\tgrad: 2.0 4.0 -0.4936504364013672 -0.9873008728027344 -0.2468252182006836\n",
      "\tgrad: 3.0 6.0 0.2685985565185547 0.8057956695556641 0.08953285217285156\n",
      "progress: 9 loss= 0.0020040329545736313\n",
      "\tgrad: 1.0 2.0 0.18941354751586914 0.18941354751586914 0.18941354751586914\n",
      "\tgrad: 2.0 4.0 -0.4933490753173828 -0.9866981506347656 -0.2466745376586914\n",
      "\tgrad: 3.0 6.0 0.26842403411865234 0.805272102355957 0.08947467803955078\n",
      "progress: 10 loss= 0.0020014294423162937\n",
      "\tgrad: 1.0 2.0 0.18931961059570312 0.18931961059570312 0.18931961059570312\n",
      "\tgrad: 2.0 4.0 -0.49304866790771484 -0.9860973358154297 -0.24652433395385742\n",
      "\tgrad: 3.0 6.0 0.26825809478759766 0.804774284362793 0.08941936492919922\n",
      "progress: 11 loss= 0.0019989556167274714\n",
      "\tgrad: 1.0 2.0 0.1892251968383789 0.1892251968383789 0.1892251968383789\n",
      "\tgrad: 2.0 4.0 -0.4927520751953125 -0.985504150390625 -0.24637603759765625\n",
      "\tgrad: 3.0 6.0 0.26808643341064453 0.8042593002319336 0.08936214447021484\n",
      "progress: 12 loss= 0.0019963982049375772\n",
      "\tgrad: 1.0 2.0 0.18913030624389648 0.18913030624389648 0.18913030624389648\n",
      "\tgrad: 2.0 4.0 -0.49245548248291016 -0.9849109649658203 -0.24622774124145508\n",
      "\tgrad: 3.0 6.0 0.2679147720336914 0.8037443161010742 0.08930492401123047\n",
      "progress: 13 loss= 0.0019938424229621887\n",
      "\tgrad: 1.0 2.0 0.18903493881225586 0.18903493881225586 0.18903493881225586\n",
      "\tgrad: 2.0 4.0 -0.4921579360961914 -0.9843158721923828 -0.2460789680480957\n",
      "\tgrad: 3.0 6.0 0.2677488327026367 0.8032464981079102 0.0892496109008789\n",
      "progress: 14 loss= 0.0019913732539862394\n",
      "\tgrad: 1.0 2.0 0.18893909454345703 0.18893909454345703 0.18893909454345703\n",
      "\tgrad: 2.0 4.0 -0.4918642044067383 -0.9837284088134766 -0.24593210220336914\n",
      "\tgrad: 3.0 6.0 0.2675771713256836 0.8027315139770508 0.08919239044189453\n",
      "progress: 15 loss= 0.001988820731639862\n",
      "\tgrad: 1.0 2.0 0.1888432502746582 0.1888432502746582 0.1888432502746582\n",
      "\tgrad: 2.0 4.0 -0.49156761169433594 -0.9831352233886719 -0.24578380584716797\n",
      "\tgrad: 3.0 6.0 0.2674140930175781 0.8022422790527344 0.08913803100585938\n",
      "progress: 16 loss= 0.001986397197470069\n",
      "\tgrad: 1.0 2.0 0.18874692916870117 0.18874692916870117 0.18874692916870117\n",
      "\tgrad: 2.0 4.0 -0.4912748336791992 -0.9825496673583984 -0.2456374168395996\n",
      "\tgrad: 3.0 6.0 0.267242431640625 0.801727294921875 0.089080810546875\n",
      "progress: 17 loss= 0.001983847701922059\n",
      "\tgrad: 1.0 2.0 0.18865060806274414 0.18865060806274414 0.18865060806274414\n",
      "\tgrad: 2.0 4.0 -0.4909801483154297 -0.9819602966308594 -0.24549007415771484\n",
      "\tgrad: 3.0 6.0 0.2670764923095703 0.8012294769287109 0.08902549743652344\n",
      "progress: 18 loss= 0.0019813848193734884\n",
      "\tgrad: 1.0 2.0 0.1885533332824707 0.1885533332824707 0.1885533332824707\n",
      "\tgrad: 2.0 4.0 -0.4906883239746094 -0.9813766479492188 -0.2453441619873047\n",
      "\tgrad: 3.0 6.0 0.26691341400146484 0.8007402420043945 0.08897113800048828\n",
      "progress: 19 loss= 0.0019789659418165684\n",
      "\tgrad: 1.0 2.0 0.18845558166503906 0.18845558166503906 0.18845558166503906\n",
      "\tgrad: 2.0 4.0 -0.49039649963378906 -0.9807929992675781 -0.24519824981689453\n",
      "\tgrad: 3.0 6.0 0.26674747467041016 0.8002424240112305 0.08891582489013672\n",
      "progress: 20 loss= 0.0019765060860663652\n",
      "\tgrad: 1.0 2.0 0.18835783004760742 0.18835783004760742 0.18835783004760742\n",
      "\tgrad: 2.0 4.0 -0.49010562896728516 -0.9802112579345703 -0.24505281448364258\n",
      "\tgrad: 3.0 6.0 0.2665843963623047 0.7997531890869141 0.08886146545410156\n",
      "progress: 21 loss= 0.001974090002477169\n",
      "\tgrad: 1.0 2.0 0.18825960159301758 0.18825960159301758 0.18825960159301758\n",
      "\tgrad: 2.0 4.0 -0.48981571197509766 -0.9796314239501953 -0.24490785598754883\n",
      "\tgrad: 3.0 6.0 0.26641845703125 0.79925537109375 0.08880615234375\n",
      "progress: 22 loss= 0.0019716331735253334\n",
      "\tgrad: 1.0 2.0 0.18816184997558594 0.18816184997558594 0.18816184997558594\n",
      "\tgrad: 2.0 4.0 -0.48952484130859375 -0.9790496826171875 -0.24476242065429688\n",
      "\tgrad: 3.0 6.0 0.26626110076904297 0.7987833023071289 0.08875370025634766\n",
      "progress: 23 loss= 0.0019693048670887947\n",
      "\tgrad: 1.0 2.0 0.1880626678466797 0.1880626678466797 0.1880626678466797\n",
      "\tgrad: 2.0 4.0 -0.4892387390136719 -0.9784774780273438 -0.24461936950683594\n",
      "\tgrad: 3.0 6.0 0.26609230041503906 0.7982769012451172 0.08869743347167969\n",
      "progress: 24 loss= 0.0019668086897581816\n",
      "\tgrad: 1.0 2.0 0.18796443939208984 0.18796443939208984 0.18796443939208984\n",
      "\tgrad: 2.0 4.0 -0.4889488220214844 -0.9778976440429688 -0.2444744110107422\n",
      "\tgrad: 3.0 6.0 0.26593780517578125 0.7978134155273438 0.08864593505859375\n",
      "progress: 25 loss= 0.0019645255524665117\n",
      "\tgrad: 1.0 2.0 0.1878652572631836 0.1878652572631836 0.1878652572631836\n",
      "\tgrad: 2.0 4.0 -0.4886627197265625 -0.977325439453125 -0.24433135986328125\n",
      "\tgrad: 3.0 6.0 0.26577186584472656 0.7973155975341797 0.08859062194824219\n",
      "progress: 26 loss= 0.0019620745442807674\n",
      "\tgrad: 1.0 2.0 0.18776607513427734 0.18776607513427734 0.18776607513427734\n",
      "\tgrad: 2.0 4.0 -0.4883747100830078 -0.9767494201660156 -0.2441873550415039\n",
      "\tgrad: 3.0 6.0 0.2656116485595703 0.7968349456787109 0.08853721618652344\n",
      "progress: 27 loss= 0.001959709683433175\n",
      "\tgrad: 1.0 2.0 0.1876668930053711 0.1876668930053711 0.1876668930053711\n",
      "\tgrad: 2.0 4.0 -0.48808860778808594 -0.9761772155761719 -0.24404430389404297\n",
      "\tgrad: 3.0 6.0 0.2654542922973633 0.7963628768920898 0.0884847640991211\n",
      "progress: 28 loss= 0.001957388361915946\n",
      "\tgrad: 1.0 2.0 0.18756675720214844 0.18756675720214844 0.18756675720214844\n",
      "\tgrad: 2.0 4.0 -0.48780345916748047 -0.9756069183349609 -0.24390172958374023\n",
      "\tgrad: 3.0 6.0 0.2652912139892578 0.7958736419677734 0.08843040466308594\n",
      "progress: 29 loss= 0.001954984152689576\n",
      "\tgrad: 1.0 2.0 0.1874675750732422 0.1874675750732422 0.1874675750732422\n",
      "\tgrad: 2.0 4.0 -0.4875164031982422 -0.9750328063964844 -0.2437582015991211\n",
      "\tgrad: 3.0 6.0 0.26513099670410156 0.7953929901123047 0.08837699890136719\n",
      "progress: 30 loss= 0.0019526234827935696\n",
      "\tgrad: 1.0 2.0 0.18736743927001953 0.18736743927001953 0.18736743927001953\n",
      "\tgrad: 2.0 4.0 -0.4872322082519531 -0.9744644165039062 -0.24361610412597656\n",
      "\tgrad: 3.0 6.0 0.26497936248779297 0.7949380874633789 0.08832645416259766\n",
      "progress: 31 loss= 0.001950390636920929\n",
      "\tgrad: 1.0 2.0 0.18726634979248047 0.18726634979248047 0.18726634979248047\n",
      "\tgrad: 2.0 4.0 -0.4869499206542969 -0.9738998413085938 -0.24347496032714844\n",
      "\tgrad: 3.0 6.0 0.26481056213378906 0.7944316864013672 0.08827018737792969\n",
      "progress: 32 loss= 0.001947906450368464\n",
      "\tgrad: 1.0 2.0 0.18716669082641602 0.18716669082641602 0.18716669082641602\n",
      "\tgrad: 2.0 4.0 -0.4866628646850586 -0.9733257293701172 -0.2433314323425293\n",
      "\tgrad: 3.0 6.0 0.26465892791748047 0.7939767837524414 0.08821964263916016\n",
      "progress: 33 loss= 0.0019456762820482254\n",
      "\tgrad: 1.0 2.0 0.18706607818603516 0.18706607818603516 0.18706607818603516\n",
      "\tgrad: 2.0 4.0 -0.48637962341308594 -0.9727592468261719 -0.24318981170654297\n",
      "\tgrad: 3.0 6.0 0.2644987106323242 0.7934961318969727 0.0881662368774414\n",
      "progress: 34 loss= 0.0019433213165029883\n",
      "\tgrad: 1.0 2.0 0.1869649887084961 0.1869649887084961 0.1869649887084961\n",
      "\tgrad: 2.0 4.0 -0.4860973358154297 -0.9721946716308594 -0.24304866790771484\n",
      "\tgrad: 3.0 6.0 0.2643470764160156 0.7930412292480469 0.08811569213867188\n",
      "progress: 35 loss= 0.0019410938257351518\n",
      "\tgrad: 1.0 2.0 0.18686389923095703 0.18686389923095703 0.18686389923095703\n",
      "\tgrad: 2.0 4.0 -0.48581790924072266 -0.9716358184814453 -0.24290895462036133\n",
      "\tgrad: 3.0 6.0 0.26418399810791016 0.7925519943237305 0.08806133270263672\n",
      "progress: 36 loss= 0.001938699628226459\n",
      "\tgrad: 1.0 2.0 0.18676376342773438 0.18676376342773438 0.18676376342773438\n",
      "\tgrad: 2.0 4.0 -0.4855327606201172 -0.9710655212402344 -0.2427663803100586\n",
      "\tgrad: 3.0 6.0 0.26402950286865234 0.792088508605957 0.08800983428955078\n",
      "progress: 37 loss= 0.001936432789079845\n",
      "\tgrad: 1.0 2.0 0.1866621971130371 0.1866621971130371 0.1866621971130371\n",
      "\tgrad: 2.0 4.0 -0.48525238037109375 -0.9705047607421875 -0.24262619018554688\n",
      "\tgrad: 3.0 6.0 0.26387786865234375 0.7916336059570312 0.08795928955078125\n",
      "progress: 38 loss= 0.0019342091400176287\n",
      "\tgrad: 1.0 2.0 0.18656063079833984 0.18656063079833984 0.18656063079833984\n",
      "\tgrad: 2.0 4.0 -0.4849739074707031 -0.9699478149414062 -0.24248695373535156\n",
      "\tgrad: 3.0 6.0 0.2637176513671875 0.7911529541015625 0.0879058837890625\n",
      "progress: 39 loss= 0.0019318611593917012\n",
      "\tgrad: 1.0 2.0 0.18646001815795898 0.18646001815795898 0.18646001815795898\n",
      "\tgrad: 2.0 4.0 -0.48469066619873047 -0.9693813323974609 -0.24234533309936523\n",
      "\tgrad: 3.0 6.0 0.2635631561279297 0.7906894683837891 0.08785438537597656\n",
      "progress: 40 loss= 0.0019295982783660293\n",
      "\tgrad: 1.0 2.0 0.18635845184326172 0.18635845184326172 0.18635845184326172\n",
      "\tgrad: 2.0 4.0 -0.4844093322753906 -0.9688186645507812 -0.2422046661376953\n",
      "\tgrad: 3.0 6.0 0.2634086608886719 0.7902259826660156 0.08780288696289062\n",
      "progress: 41 loss= 0.0019273367943242192\n",
      "\tgrad: 1.0 2.0 0.18625736236572266 0.18625736236572266 0.18625736236572266\n",
      "\tgrad: 2.0 4.0 -0.484130859375 -0.96826171875 -0.2420654296875\n",
      "\tgrad: 3.0 6.0 0.26325416564941406 0.7897624969482422 0.08775138854980469\n",
      "progress: 42 loss= 0.0019250765908509493\n",
      "\tgrad: 1.0 2.0 0.1861557960510254 0.1861557960510254 0.1861557960510254\n",
      "\tgrad: 2.0 4.0 -0.48385143280029297 -0.9677028656005859 -0.24192571640014648\n",
      "\tgrad: 3.0 6.0 0.26309680938720703 0.7892904281616211 0.08769893646240234\n",
      "progress: 43 loss= 0.0019227758748456836\n",
      "\tgrad: 1.0 2.0 0.18605422973632812 0.18605422973632812 0.18605422973632812\n",
      "\tgrad: 2.0 4.0 -0.48357105255126953 -0.9671421051025391 -0.24178552627563477\n",
      "\tgrad: 3.0 6.0 0.26294803619384766 0.788844108581543 0.08764934539794922\n",
      "progress: 44 loss= 0.0019206019351258874\n",
      "\tgrad: 1.0 2.0 0.18595218658447266 0.18595218658447266 0.18595218658447266\n",
      "\tgrad: 2.0 4.0 -0.4832954406738281 -0.9665908813476562 -0.24164772033691406\n",
      "\tgrad: 3.0 6.0 0.2627906799316406 0.7883720397949219 0.08759689331054688\n",
      "progress: 45 loss= 0.0019183038966730237\n",
      "\tgrad: 1.0 2.0 0.1858506202697754 0.1858506202697754 0.1858506202697754\n",
      "\tgrad: 2.0 4.0 -0.4830150604248047 -0.9660301208496094 -0.24150753021240234\n",
      "\tgrad: 3.0 6.0 0.2626361846923828 0.7879085540771484 0.08754539489746094\n",
      "progress: 46 loss= 0.0019160490483045578\n",
      "\tgrad: 1.0 2.0 0.18574857711791992 0.18574857711791992 0.18574857711791992\n",
      "\tgrad: 2.0 4.0 -0.48273754119873047 -0.9654750823974609 -0.24136877059936523\n",
      "\tgrad: 3.0 6.0 0.26248741149902344 0.7874622344970703 0.08749580383300781\n",
      "progress: 47 loss= 0.001913878950290382\n",
      "\tgrad: 1.0 2.0 0.18564653396606445 0.18564653396606445 0.18564653396606445\n",
      "\tgrad: 2.0 4.0 -0.48246097564697266 -0.9649219512939453 -0.24123048782348633\n",
      "\tgrad: 3.0 6.0 0.2623300552368164 0.7869901657104492 0.08744335174560547\n",
      "progress: 48 loss= 0.0019115849863737822\n",
      "\tgrad: 1.0 2.0 0.1855449676513672 0.1855449676513672 0.1855449676513672\n",
      "\tgrad: 2.0 4.0 -0.48218250274658203 -0.9643650054931641 -0.24109125137329102\n",
      "\tgrad: 3.0 6.0 0.2621784210205078 0.7865352630615234 0.08739280700683594\n",
      "progress: 49 loss= 0.0019093756563961506\n",
      "\tgrad: 1.0 2.0 0.18544244766235352 0.18544244766235352 0.18544244766235352\n",
      "\tgrad: 2.0 4.0 -0.4819059371948242 -0.9638118743896484 -0.2409529685974121\n",
      "\tgrad: 3.0 6.0 0.2620267868041992 0.7860803604125977 0.0873422622680664\n",
      "progress: 50 loss= 0.001907167723402381\n",
      "\tgrad: 1.0 2.0 0.18534040451049805 0.18534040451049805 0.18534040451049805\n",
      "\tgrad: 2.0 4.0 -0.4816293716430664 -0.9632587432861328 -0.2408146858215332\n",
      "\tgrad: 3.0 6.0 0.2618722915649414 0.7856168746948242 0.08729076385498047\n",
      "progress: 51 loss= 0.0019049193942919374\n",
      "\tgrad: 1.0 2.0 0.18523883819580078 0.18523883819580078 0.18523883819580078\n",
      "\tgrad: 2.0 4.0 -0.4813518524169922 -0.9627037048339844 -0.2406759262084961\n",
      "\tgrad: 3.0 6.0 0.26172351837158203 0.7851705551147461 0.08724117279052734\n",
      "progress: 52 loss= 0.0019027555827051401\n",
      "\tgrad: 1.0 2.0 0.1851358413696289 0.1851358413696289 0.1851358413696289\n",
      "\tgrad: 2.0 4.0 -0.4810771942138672 -0.9621543884277344 -0.2405385971069336\n",
      "\tgrad: 3.0 6.0 0.26157474517822266 0.784724235534668 0.08719158172607422\n",
      "progress: 53 loss= 0.0019005929352715611\n",
      "\tgrad: 1.0 2.0 0.18503379821777344 0.18503379821777344 0.18503379821777344\n",
      "\tgrad: 2.0 4.0 -0.4808025360107422 -0.9616050720214844 -0.2404012680053711\n",
      "\tgrad: 3.0 6.0 0.2614173889160156 0.7842521667480469 0.08713912963867188\n",
      "progress: 54 loss= 0.0018983070040121675\n",
      "\tgrad: 1.0 2.0 0.18493175506591797 0.18493175506591797 0.18493175506591797\n",
      "\tgrad: 2.0 4.0 -0.48052501678466797 -0.9610500335693359 -0.24026250839233398\n",
      "\tgrad: 3.0 6.0 0.26126861572265625 0.7838058471679688 0.08708953857421875\n",
      "progress: 55 loss= 0.0018961469177156687\n",
      "\tgrad: 1.0 2.0 0.1848297119140625 0.1848297119140625 0.1848297119140625\n",
      "\tgrad: 2.0 4.0 -0.4802513122558594 -0.9605026245117188 -0.2401256561279297\n",
      "\tgrad: 3.0 6.0 0.26111412048339844 0.7833423614501953 0.08703804016113281\n",
      "progress: 56 loss= 0.0018939051078632474\n",
      "\tgrad: 1.0 2.0 0.18472671508789062 0.18472671508789062 0.18472671508789062\n",
      "\tgrad: 2.0 4.0 -0.47997570037841797 -0.9599514007568359 -0.23998785018920898\n",
      "\tgrad: 3.0 6.0 0.2609710693359375 0.7829132080078125 0.0869903564453125\n",
      "progress: 57 loss= 0.0018918304704129696\n",
      "\tgrad: 1.0 2.0 0.18462371826171875 0.18462371826171875 0.18462371826171875\n",
      "\tgrad: 2.0 4.0 -0.4797048568725586 -0.9594097137451172 -0.2398524284362793\n",
      "\tgrad: 3.0 6.0 0.2608165740966797 0.7824497222900391 0.08693885803222656\n",
      "progress: 58 loss= 0.0018895912216976285\n",
      "\tgrad: 1.0 2.0 0.18452167510986328 0.18452167510986328 0.18452167510986328\n",
      "\tgrad: 2.0 4.0 -0.4794292449951172 -0.9588584899902344 -0.2397146224975586\n",
      "\tgrad: 3.0 6.0 0.2606620788574219 0.7819862365722656 0.08688735961914062\n",
      "progress: 59 loss= 0.0018873533699661493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgrad: 1.0 2.0 0.1844196319580078 0.1844196319580078 0.1844196319580078\n",
      "\tgrad: 2.0 4.0 -0.4791526794433594 -0.9583053588867188 -0.2395763397216797\n",
      "\tgrad: 3.0 6.0 0.26051902770996094 0.7815570831298828 0.08683967590332031\n",
      "progress: 60 loss= 0.0018852823413908482\n",
      "\tgrad: 1.0 2.0 0.18431711196899414 0.18431711196899414 0.18431711196899414\n",
      "\tgrad: 2.0 4.0 -0.4788818359375 -0.957763671875 -0.23944091796875\n",
      "\tgrad: 3.0 6.0 0.2603645324707031 0.7810935974121094 0.08678817749023438\n",
      "progress: 61 loss= 0.0018830469343811274\n",
      "\tgrad: 1.0 2.0 0.18421506881713867 0.18421506881713867 0.18421506881713867\n",
      "\tgrad: 2.0 4.0 -0.478607177734375 -0.95721435546875 -0.2393035888671875\n",
      "\tgrad: 3.0 6.0 0.26021575927734375 0.7806472778320312 0.08673858642578125\n",
      "progress: 62 loss= 0.0018808955792337656\n",
      "\tgrad: 1.0 2.0 0.184112548828125 0.184112548828125 0.184112548828125\n",
      "\tgrad: 2.0 4.0 -0.4783334732055664 -0.9566669464111328 -0.2391667366027832\n",
      "\tgrad: 3.0 6.0 0.2600669860839844 0.7802009582519531 0.08668899536132812\n",
      "progress: 63 loss= 0.001878745504654944\n",
      "\tgrad: 1.0 2.0 0.18401050567626953 0.18401050567626953 0.18401050567626953\n",
      "\tgrad: 2.0 4.0 -0.4780616760253906 -0.9561233520507812 -0.2390308380126953\n",
      "\tgrad: 3.0 6.0 0.259918212890625 0.779754638671875 0.086639404296875\n",
      "progress: 64 loss= 0.0018765965942293406\n",
      "\tgrad: 1.0 2.0 0.18390750885009766 0.18390750885009766 0.18390750885009766\n",
      "\tgrad: 2.0 4.0 -0.47778892517089844 -0.9555778503417969 -0.23889446258544922\n",
      "\tgrad: 3.0 6.0 0.2597665786743164 0.7792997360229492 0.08658885955810547\n",
      "progress: 65 loss= 0.0018744076369330287\n",
      "\tgrad: 1.0 2.0 0.1838054656982422 0.1838054656982422 0.1838054656982422\n",
      "\tgrad: 2.0 4.0 -0.47751617431640625 -0.9550323486328125 -0.23875808715820312\n",
      "\tgrad: 3.0 6.0 0.25961780548095703 0.7788534164428711 0.08653926849365234\n",
      "progress: 66 loss= 0.0018722612876445055\n",
      "\tgrad: 1.0 2.0 0.18370294570922852 0.18370294570922852 0.18370294570922852\n",
      "\tgrad: 2.0 4.0 -0.47724342346191406 -0.9544868469238281 -0.23862171173095703\n",
      "\tgrad: 3.0 6.0 0.2594718933105469 0.7784156799316406 0.08649063110351562\n",
      "progress: 67 loss= 0.0018701573135331273\n",
      "\tgrad: 1.0 2.0 0.18359994888305664 0.18359994888305664 0.18359994888305664\n",
      "\tgrad: 2.0 4.0 -0.4769725799560547 -0.9539451599121094 -0.23848628997802734\n",
      "\tgrad: 3.0 6.0 0.25931453704833984 0.7779436111450195 0.08643817901611328\n",
      "progress: 68 loss= 0.0018678896594792604\n",
      "\tgrad: 1.0 2.0 0.18349838256835938 0.18349838256835938 0.18349838256835938\n",
      "\tgrad: 2.0 4.0 -0.4766979217529297 -0.9533958435058594 -0.23834896087646484\n",
      "\tgrad: 3.0 6.0 0.2591743469238281 0.7775230407714844 0.08639144897460938\n",
      "progress: 69 loss= 0.001865870668552816\n",
      "\tgrad: 1.0 2.0 0.1833953857421875 0.1833953857421875 0.1833953857421875\n",
      "\tgrad: 2.0 4.0 -0.4764289855957031 -0.9528579711914062 -0.23821449279785156\n",
      "\tgrad: 3.0 6.0 0.25902271270751953 0.7770681381225586 0.08634090423583984\n",
      "progress: 70 loss= 0.0018636878812685609\n",
      "\tgrad: 1.0 2.0 0.18329334259033203 0.18329334259033203 0.18329334259033203\n",
      "\tgrad: 2.0 4.0 -0.47615718841552734 -0.9523143768310547 -0.23807859420776367\n",
      "\tgrad: 3.0 6.0 0.25887393951416016 0.7766218185424805 0.08629131317138672\n",
      "progress: 71 loss= 0.0018615477019920945\n",
      "\tgrad: 1.0 2.0 0.18319034576416016 0.18319034576416016 0.18319034576416016\n",
      "\tgrad: 2.0 4.0 -0.47588539123535156 -0.9517707824707031 -0.23794269561767578\n",
      "\tgrad: 3.0 6.0 0.2587308883666992 0.7761926651000977 0.0862436294555664\n",
      "progress: 72 loss= 0.0018594908760860562\n",
      "\tgrad: 1.0 2.0 0.18308782577514648 0.18308782577514648 0.18308782577514648\n",
      "\tgrad: 2.0 4.0 -0.4756183624267578 -0.9512367248535156 -0.2378091812133789\n",
      "\tgrad: 3.0 6.0 0.2585763931274414 0.7757291793823242 0.08619213104248047\n",
      "progress: 73 loss= 0.0018572708358988166\n",
      "\tgrad: 1.0 2.0 0.18298578262329102 0.18298578262329102 0.18298578262329102\n",
      "\tgrad: 2.0 4.0 -0.4753446578979492 -0.9506893157958984 -0.2376723289489746\n",
      "\tgrad: 3.0 6.0 0.25843334197998047 0.7753000259399414 0.08614444732666016\n",
      "progress: 74 loss= 0.0018552164547145367\n",
      "\tgrad: 1.0 2.0 0.18288326263427734 0.18288326263427734 0.18288326263427734\n",
      "\tgrad: 2.0 4.0 -0.47507476806640625 -0.9501495361328125 -0.23753738403320312\n",
      "\tgrad: 3.0 6.0 0.2582845687866211 0.7748537063598633 0.08609485626220703\n",
      "progress: 75 loss= 0.0018530810484662652\n",
      "\tgrad: 1.0 2.0 0.18278121948242188 0.18278121948242188 0.18278121948242188\n",
      "\tgrad: 2.0 4.0 -0.4748058319091797 -0.9496116638183594 -0.23740291595458984\n",
      "\tgrad: 3.0 6.0 0.25813865661621094 0.7744159698486328 0.08604621887207031\n",
      "progress: 76 loss= 0.001850987900979817\n",
      "\tgrad: 1.0 2.0 0.1826786994934082 0.1826786994934082 0.1826786994934082\n",
      "\tgrad: 2.0 4.0 -0.4745349884033203 -0.9490699768066406 -0.23726749420166016\n",
      "\tgrad: 3.0 6.0 0.25798702239990234 0.773961067199707 0.08599567413330078\n",
      "progress: 77 loss= 0.0018488139612600207\n",
      "\tgrad: 1.0 2.0 0.18257665634155273 0.18257665634155273 0.18257665634155273\n",
      "\tgrad: 2.0 4.0 -0.47426605224609375 -0.9485321044921875 -0.23713302612304688\n",
      "\tgrad: 3.0 6.0 0.2578411102294922 0.7735233306884766 0.08594703674316406\n",
      "progress: 78 loss= 0.0018467232584953308\n",
      "\tgrad: 1.0 2.0 0.18247413635253906 0.18247413635253906 0.18247413635253906\n",
      "\tgrad: 2.0 4.0 -0.4739952087402344 -0.9479904174804688 -0.2369976043701172\n",
      "\tgrad: 3.0 6.0 0.25770092010498047 0.7731027603149414 0.08590030670166016\n",
      "progress: 79 loss= 0.0018447156762704253\n",
      "\tgrad: 1.0 2.0 0.1823711395263672 0.1823711395263672 0.1823711395263672\n",
      "\tgrad: 2.0 4.0 -0.47372913360595703 -0.9474582672119141 -0.23686456680297852\n",
      "\tgrad: 3.0 6.0 0.25754356384277344 0.7726306915283203 0.08584785461425781\n",
      "progress: 80 loss= 0.0018424635054543614\n",
      "\tgrad: 1.0 2.0 0.18226957321166992 0.18226957321166992 0.18226957321166992\n",
      "\tgrad: 2.0 4.0 -0.47345733642578125 -0.9469146728515625 -0.23672866821289062\n",
      "\tgrad: 3.0 6.0 0.2574005126953125 0.7722015380859375 0.0858001708984375\n",
      "progress: 81 loss= 0.0018404172733426094\n",
      "\tgrad: 1.0 2.0 0.18216705322265625 0.18216705322265625 0.18216705322265625\n",
      "\tgrad: 2.0 4.0 -0.4731884002685547 -0.9463768005371094 -0.23659420013427734\n",
      "\tgrad: 3.0 6.0 0.25725746154785156 0.7717723846435547 0.08575248718261719\n",
      "progress: 82 loss= 0.0018383723217993975\n",
      "\tgrad: 1.0 2.0 0.18206501007080078 0.18206501007080078 0.18206501007080078\n",
      "\tgrad: 2.0 4.0 -0.47292137145996094 -0.9458427429199219 -0.23646068572998047\n",
      "\tgrad: 3.0 6.0 0.25710582733154297 0.7713174819946289 0.08570194244384766\n",
      "progress: 83 loss= 0.0018362057162448764\n",
      "\tgrad: 1.0 2.0 0.18196344375610352 0.18196344375610352 0.18196344375610352\n",
      "\tgrad: 2.0 4.0 -0.47265052795410156 -0.9453010559082031 -0.23632526397705078\n",
      "\tgrad: 3.0 6.0 0.25696277618408203 0.7708883285522461 0.08565425872802734\n",
      "progress: 84 loss= 0.0018341629765927792\n",
      "\tgrad: 1.0 2.0 0.18186092376708984 0.18186092376708984 0.18186092376708984\n",
      "\tgrad: 2.0 4.0 -0.4723844528198242 -0.9447689056396484 -0.2361922264099121\n",
      "\tgrad: 3.0 6.0 0.25681114196777344 0.7704334259033203 0.08560371398925781\n",
      "progress: 85 loss= 0.0018319989321753383\n",
      "\tgrad: 1.0 2.0 0.18175888061523438 0.18175888061523438 0.18175888061523438\n",
      "\tgrad: 2.0 4.0 -0.47211456298828125 -0.9442291259765625 -0.23605728149414062\n",
      "\tgrad: 3.0 6.0 0.25667381286621094 0.7700214385986328 0.08555793762207031\n",
      "progress: 86 loss= 0.0018300401279702783\n",
      "\tgrad: 1.0 2.0 0.1816563606262207 0.1816563606262207 0.1816563606262207\n",
      "\tgrad: 2.0 4.0 -0.4718494415283203 -0.9436988830566406 -0.23592472076416016\n",
      "\tgrad: 3.0 6.0 0.25652503967285156 0.7695751190185547 0.08550834655761719\n",
      "progress: 87 loss= 0.001827919390052557\n",
      "\tgrad: 1.0 2.0 0.18155479431152344 0.18155479431152344 0.18155479431152344\n",
      "\tgrad: 2.0 4.0 -0.47158241271972656 -0.9431648254394531 -0.23579120635986328\n",
      "\tgrad: 3.0 6.0 0.25637340545654297 0.7691202163696289 0.08545780181884766\n",
      "progress: 88 loss= 0.0018257589545100927\n",
      "\tgrad: 1.0 2.0 0.18145322799682617 0.18145322799682617 0.18145322799682617\n",
      "\tgrad: 2.0 4.0 -0.4713115692138672 -0.9426231384277344 -0.2356557846069336\n",
      "\tgrad: 3.0 6.0 0.25623321533203125 0.7686996459960938 0.08541107177734375\n",
      "progress: 89 loss= 0.0018237627809867263\n",
      "\tgrad: 1.0 2.0 0.1813507080078125 0.1813507080078125 0.1813507080078125\n",
      "\tgrad: 2.0 4.0 -0.47104644775390625 -0.9420928955078125 -0.23552322387695312\n",
      "\tgrad: 3.0 6.0 0.2560844421386719 0.7682533264160156 0.08536148071289062\n",
      "progress: 90 loss= 0.0018216456519439816\n",
      "\tgrad: 1.0 2.0 0.18124866485595703 0.18124866485595703 0.18124866485595703\n",
      "\tgrad: 2.0 4.0 -0.4707775115966797 -0.9415550231933594 -0.23538875579833984\n",
      "\tgrad: 3.0 6.0 0.25594139099121094 0.7678241729736328 0.08531379699707031\n",
      "progress: 91 loss= 0.0018196109449490905\n",
      "\tgrad: 1.0 2.0 0.18114709854125977 0.18114709854125977 0.18114709854125977\n",
      "\tgrad: 2.0 4.0 -0.47051239013671875 -0.9410247802734375 -0.23525619506835938\n",
      "\tgrad: 3.0 6.0 0.25579261779785156 0.7673778533935547 0.08526420593261719\n",
      "progress: 92 loss= 0.0018174962606281042\n",
      "\tgrad: 1.0 2.0 0.1810455322265625 0.1810455322265625 0.1810455322265625\n",
      "\tgrad: 2.0 4.0 -0.4702434539794922 -0.9404869079589844 -0.2351217269897461\n",
      "\tgrad: 3.0 6.0 0.2556495666503906 0.7669486999511719 0.08521652221679688\n",
      "progress: 93 loss= 0.0018154638819396496\n",
      "\tgrad: 1.0 2.0 0.18094348907470703 0.18094348907470703 0.18094348907470703\n",
      "\tgrad: 2.0 4.0 -0.46997833251953125 -0.9399566650390625 -0.23498916625976562\n",
      "\tgrad: 3.0 6.0 0.25550365447998047 0.7665109634399414 0.08516788482666016\n",
      "progress: 94 loss= 0.0018133921548724174\n",
      "\tgrad: 1.0 2.0 0.18084192276000977 0.18084192276000977 0.18084192276000977\n",
      "\tgrad: 2.0 4.0 -0.4697113037109375 -0.939422607421875 -0.23485565185546875\n",
      "\tgrad: 3.0 6.0 0.2553577423095703 0.7660732269287109 0.08511924743652344\n",
      "progress: 95 loss= 0.0018113215919584036\n",
      "\tgrad: 1.0 2.0 0.1807403564453125 0.1807403564453125 0.1807403564453125\n",
      "\tgrad: 2.0 4.0 -0.46944522857666016 -0.9388904571533203 -0.23472261428833008\n",
      "\tgrad: 3.0 6.0 0.2552146911621094 0.7656440734863281 0.08507156372070312\n",
      "progress: 96 loss= 0.0018092927057296038\n",
      "\tgrad: 1.0 2.0 0.18063831329345703 0.18063831329345703 0.18063831329345703\n",
      "\tgrad: 2.0 4.0 -0.4691801071166992 -0.9383602142333984 -0.2345900535583496\n",
      "\tgrad: 3.0 6.0 0.2550687789916992 0.7652063369750977 0.0850229263305664\n",
      "progress: 97 loss= 0.0018072244711220264\n",
      "\tgrad: 1.0 2.0 0.18053627014160156 0.18053627014160156 0.18053627014160156\n",
      "\tgrad: 2.0 4.0 -0.4689140319824219 -0.9378280639648438 -0.23445701599121094\n",
      "\tgrad: 3.0 6.0 0.2549257278442383 0.7647771835327148 0.0849752426147461\n",
      "progress: 98 loss= 0.0018051979131996632\n",
      "\tgrad: 1.0 2.0 0.1804347038269043 0.1804347038269043 0.1804347038269043\n",
      "\tgrad: 2.0 4.0 -0.46864891052246094 -0.9372978210449219 -0.23432445526123047\n",
      "\tgrad: 3.0 6.0 0.2547769546508789 0.7643308639526367 0.08492565155029297\n",
      "progress: 99 loss= 0.0018030916107818484\n",
      "predict (before training) 4 8.299124717712402\n"
     ]
    }
   ],
   "source": [
    "# before training\n",
    "print(\"predict (before training)\", 4, forward(4).data[0])\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        l = loss(x_val, y_val)\n",
    "        l.backward()\n",
    "        print(\"\\tgrad:\", x_val, y_val, w1.grad.data[0], w2.grad.data[0], b.grad.data[0])\n",
    "        w1.data = w1.data - learning_rate * w1.grad.data\n",
    "        w2.data = w2.data - learning_rate * w2.grad.data\n",
    "        b.data = b.data - learning_rate * b.grad.data\n",
    "        \n",
    "        # reset gradient\n",
    "        w1.grad.data.zero_()\n",
    "        w2.grad.data.zero_()\n",
    "        b.grad.data.zero_()\n",
    "        \n",
    "    print(\"progress:\", epoch, \"loss=\", l.data[0])\n",
    "\n",
    "# After training\n",
    "print(\"predict (before training)\", 4, forward(4).data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
